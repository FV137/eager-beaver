# Eager Beaver ğŸ¦«

**Complete toolkit for vision AI workflows** - from photo organization to custom model training.

## âœ¨ **NEW: Beautiful TUI Wizard**

**The easiest way to train LoRAs!** Guided workflow with model presets (SDXL, Flux.1, ZIT) and smart defaults.

```bash
python beaver.py
```

Features:
- ğŸ¨ Beautiful terminal interface with progress tracking
- âš™ï¸ Model-specific presets (SDXL, Flux.1, ZIT, SD 1.5, SD 2.1)
- âš ï¸ Quality warnings (shot distribution, dataset size)
- ğŸ’¾ Session save/resume
- ğŸš€ Complete guided workflow in one command

**[Try it now: `python beaver.py`](#guided-wizard-recommended)**

---

## What's Inside

This monorepo contains four powerful tools that work great together or standalone:

### ğŸ—‚ï¸ **FaceVault** - Photo Organization
Beautiful face organization for your photo library. Perfect for Christmas photos, family albums, or any collection with people.

```bash
python facevault.py scan ~/Pictures/Christmas2024
python facevault.py cluster --preview
python facevault.py label
python facevault.py export --all --format organized
```

**[ğŸ“– Full FaceVault Documentation â†’](FACEVAULT.md)**

### ğŸ¨ **LoRA Prep** - Intelligent Dataset Preparation
Prepare LoRA training datasets with automatic shot classification (close/mid/far) and concept extraction.

```bash
python lora_prep.py prepare /path/to/photos \
  --name "Emma" \
  --facevault-cache outputs/facevault/face_cache.json \
  --captions outputs/processed/emma/captions.json
```

**[ğŸ“– Full LoRA Workflow Guide â†’](LORA_WORKFLOW.md)**

### ğŸš€ **LoRA Train** - Simplified Training Integration
Configure LoRA training with smart defaults. Generates configs for kohya_ss, diffusers, and more.

```bash
python lora_train.py train outputs/lora_datasets/emma --interactive
# Auto-calculates epochs, batch size, learning rate
# Generates ready-to-run training scripts
```

**[ğŸ“– Full LoRA Workflow Guide â†’](LORA_WORKFLOW.md)**

### ğŸ—ï¸ **Vision Training Pipeline**
End-to-end pipeline for training vision models with custom datasets and precise vocabulary control.

```bash
# Pull datasets (HuggingFace, local, or organized with FaceVault)
python scripts/download_datasets.py

# Caption with vision models
python scripts/caption_images.py --dataset all

# Train your own vision model
# (Supports general datasets, not just NSFW)
```

---

## Quick Start

### Installation

```bash
# Clone the repo
git clone <repo-url> eager-beaver
cd eager-beaver

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Login to HuggingFace (for model downloads)
huggingface-cli login
```

### Guided Wizard (Recommended)

**The easiest way to get started!**

```bash
python beaver.py
```

Interactive menu:
1. **LoRA Training Pipeline** - Complete workflow with model presets
2. **Photo Organization** - Just organize photos (FaceVault only)
3. **Individual Tools** - Run tools manually

**Model Presets Included:**
- ğŸ¨ **SDXL** - Standard Stable Diffusion XL (1024px, lr=1e-4, dim=32)
- âš¡ **Flux.1** - Black Forest Labs Flux (1024px, lr=4e-4, dim=64)
- ğŸ¯ **ZIT (ZTSNR)** - Zero Terminal SNR (1024px, lr=8e-5, special SNR settings)
- ğŸ“¸ **SD 1.5** - Classic Stable Diffusion (512px, lr=1e-4, dim=32)
- ğŸ–¼ï¸ **SD 2.1** - SD 2.1 with v-param (768px, lr=1e-4, dim=32)
- âš™ï¸ **Custom** - Start with SDXL defaults, customize as needed

**Quality Warnings:**
The wizard shows yellow bubble warnings for:
- Low image count (<15 images)
- Unbalanced shot distribution (too many close-ups, not enough variety)
- Missing face classifications

---

### Manual Workflows (Advanced)

If you prefer running tools individually:

#### **Workflow A: Photo Organization Only**

Use FaceVault to organize your personal photo library:

```bash
# Scan and organize
python facevault.py scan ~/Pictures
python facevault.py cluster
python facevault.py label
python facevault.py export --all --format organized

# See stats
python facevault.py stats
```

#### **Workflow B: Vision Model Training**

Train a custom vision model on any dataset:

```bash
# Download datasets from HuggingFace
python scripts/download_datasets.py

# Caption images with Qwen3-VL
python scripts/caption_images.py --dataset all

# Prepare training data
python scripts/prepare_training_data.py --dataset all --merge

# Upload to HuggingFace
python scripts/upload_dataset.py --dataset combined --repo-id your-username/dataset-name

# Train via HuggingFace (or local)
# Fine-tune on your captioned dataset
```

#### **Workflow C: Personal LoRA Training (The "Both" Option)** â­

Organize your photos with FaceVault, then train personalized LoRA models with intelligent shot classification:

```bash
# 1. Organize your photo library
python facevault.py scan ~/Pictures/Family
python facevault.py cluster --threshold 0.35
python facevault.py label

# 2. Export person
python facevault.py export --person person_001 --format organized

# 3. Caption the images
python scripts/caption_images.py --dataset custom \
  --input outputs/facevault/organized/Emma \
  --output-name emma

# 4. Prepare LoRA dataset with shot classification
python lora_prep.py prepare outputs/facevault/organized/Emma \
  --name "Emma" \
  --facevault-cache outputs/facevault/face_cache.json \
  --captions outputs/processed/emma/captions.json \
  --taxonomy configs/taxonomy.json

# 5. Configure training with smart defaults
python lora_train.py train outputs/lora_datasets/emma --interactive

# 6. Train LoRA (multiple options)
# - Edit and run: outputs/lora_models/emma/.../train.sh
# - Use kohya_ss: python train_network.py --config kohya_config.json
# - Use diffusers: See generated train.sh for examples
```

**What you get:**
- Images organized by shot type (close-up, mid-range, full-body)
- Extracted concepts from captions (poses, settings, clothing)
- Auto-calculated training parameters (epochs, batch size, learning rate)
- Ready-to-run configs for kohya_ss, diffusers, and more
- Complete pipeline: photos â†’ trained LoRA

---

## Project Structure

```
eager-beaver/
â”œâ”€â”€ beaver.py                   # âœ¨ TUI wizard (RECOMMENDED!)
â”œâ”€â”€ facevault.py                # ğŸ—‚ï¸  Face organization CLI
â”œâ”€â”€ FACEVAULT.md               # Documentation for FaceVault
â”œâ”€â”€ lora_prep.py               # ğŸ¨ LoRA dataset preparation
â”œâ”€â”€ lora_train.py              # ğŸš€ LoRA training integration
â”œâ”€â”€ LORA_WORKFLOW.md           # LoRA training workflow guide
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ taxonomy.json          # Vocabulary for captioning
â”‚   â””â”€â”€ model_presets.json     # Model-specific training presets
â”‚
â”œâ”€â”€ scripts/                    # ğŸ—ï¸  Vision training pipeline
â”‚   â”œâ”€â”€ download_datasets.py   # Pull datasets from HuggingFace
â”‚   â”œâ”€â”€ caption_images.py      # Caption with Qwen3-VL (or custom model)
â”‚   â”œâ”€â”€ prepare_training_data.py   # Convert to training format
â”‚   â”œâ”€â”€ upload_dataset.py      # Upload to HuggingFace Hub
â”‚   â”œâ”€â”€ test_captioner.py      # Test captioning models
â”‚   â””â”€â”€ organize_faces.py      # (Legacy - use facevault.py)
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ taxonomy.json          # Vocabulary for captioning (customizable)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ facevault/             # FaceVault outputs
â”‚   â”‚   â”œâ”€â”€ organized/         # Organized by person
â”‚   â”‚   â””â”€â”€ face_cache.json    # Face detection cache
â”‚   â”œâ”€â”€ lora_datasets/         # LoRA training datasets
â”‚   â”‚   â””â”€â”€ <person>/
â”‚   â”‚       â”œâ”€â”€ close/         # Close-up shots
â”‚   â”‚       â”œâ”€â”€ mid/           # Mid-range shots
â”‚   â”‚       â”œâ”€â”€ far/           # Full-body shots
â”‚   â”‚       â”œâ”€â”€ captions/      # Text captions
â”‚   â”‚       â””â”€â”€ metadata.json  # Dataset info
â”‚   â”œâ”€â”€ lora_models/           # LoRA training configs
â”‚   â”‚   â””â”€â”€ <person>/
â”‚   â”‚       â””â”€â”€ <timestamp>/
â”‚   â”‚           â”œâ”€â”€ kohya_config.json
â”‚   â”‚           â”œâ”€â”€ training_config.json
â”‚   â”‚           â””â”€â”€ train.sh
â”‚   â””â”€â”€ faces/                 # Legacy organize_faces.py output
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Downloaded datasets
â”‚   â”œâ”€â”€ processed/             # Captioned datasets
â”‚   â””â”€â”€ training_ready/        # Final training format
â”‚
â”œâ”€â”€ requirements.txt           # All dependencies
â””â”€â”€ README.md                  # This file
```

---

## Tools Deep Dive

### ğŸ—‚ï¸ FaceVault

**Beautiful terminal-based face organization.**

#### Features:
- âœ¨ Simple 4-step workflow: scan â†’ cluster â†’ label â†’ export
- ğŸ¨ Rich terminal UI with progress bars and previews
- ğŸš€ Resume support (safe to interrupt anytime)
- ğŸ¯ Quality filtering (min confidence, min face size)
- ğŸ“¦ Multiple export formats (organized folders, LoRA-ready, JSON)
- ğŸ”— Symlink support (save disk space)

#### Commands:
```bash
facevault.py scan      # Extract faces from photos
facevault.py cluster   # Group similar faces
facevault.py label     # Name your clusters interactively
facevault.py export    # Export in various formats
facevault.py stats     # View statistics
```

**[ğŸ“– Full Documentation](FACEVAULT.md)** | **[Examples](FACEVAULT.md#complete-workflow-example)**

---

### ğŸ¨ Vision Training Pipeline

**Complete pipeline for vision model fine-tuning.**

#### Dataset Support

**Not just NSFW!** This pipeline works with any image dataset:
- Personal photos (via FaceVault)
- Art collections
- Product photography
- Scientific imagery
- HuggingFace datasets
- Local directories

The included `taxonomy.json` is just one example - create your own for any domain.

#### Pipeline Steps

**1. Download Datasets**

```bash
# Download from HuggingFace
python scripts/download_datasets.py

# Datasets saved to data/raw/
```

Currently configured for:
- `wallstoneai/civitai-top-nsfw-images-with-metadata` (~6K images)
- `zxbsmk/NSFW-T2I` (~38K images)

*Edit the script to add your own HuggingFace datasets.*

**2. Caption Images**

```bash
# Caption all datasets
python scripts/caption_images.py --dataset all

# Caption specific dataset
python scripts/caption_images.py --dataset civitai

# Caption custom directory
python scripts/caption_images.py --dataset custom \
  --input /path/to/images \
  --output-name my_dataset

# Use different model
python scripts/caption_images.py --model "another/vision-model"
```

**Default model:** `Disty0/Qwen3-VL-8B-NSFW-Caption-V4.5`
- Automatic VRAM detection (bf16 on 24GB+, 4-bit on smaller GPUs)
- Resume support (skips already captioned)
- Uses taxonomy from `configs/taxonomy.json`

**3. Prepare Training Data**

```bash
# Prepare all datasets
python scripts/prepare_training_data.py --dataset all --merge

# Prepare specific dataset
python scripts/prepare_training_data.py --dataset civitai
```

Converts captions to conversation format for vision model training.
Output: `data/training_ready/`

**4. Upload to HuggingFace**

```bash
python scripts/upload_dataset.py \
  --dataset combined \
  --repo-id your-username/dataset-name
```

**5. Train**

Use HuggingFace AutoTrain, local training scripts, or cloud platforms.

**Example for Ministral 3 14B:**
```python
# Fine-tune mistralai/Ministral-3-14B-Base-2512
# on your-username/dataset-name
# for vision captioning using full fine-tuning on A100
```

---

## Customization

### Vocabulary Taxonomy

Edit `configs/taxonomy.json` to customize captioning vocabulary for your domain.

**Current categories (customizable):**
- `clothing_lower`, `clothing_upper`, `clothing_full`
- `swimwear`, `lingerie`, `accessories`
- `body_descriptors`, `poses`, `expressions`
- `lighting`, `settings`

**Example custom taxonomy for art:**
```json
{
  "art_style": ["impressionist", "abstract", "realistic", ...],
  "medium": ["oil painting", "watercolor", "digital", ...],
  "composition": ["rule of thirds", "centered", "asymmetric", ...],
  "color_palette": ["warm", "cool", "monochrome", ...]
}
```

### Caption Quality Filters

Adjust in `scripts/prepare_training_data.py`:
- `min_caption_length` - Minimum characters (default: 50)
- `max_caption_length` - Maximum characters (default: 2000)

### Face Detection Quality

Adjust in FaceVault scan:
```bash
# High quality only
python facevault.py scan /path --min-score 0.7 --min-size 100

# Catch everything
python facevault.py scan /path --min-score 0.3 --min-size 30
```

---

## Hardware Requirements

| Task | GPU | VRAM | Notes |
|------|-----|------|-------|
| FaceVault (scan) | RTX 3060+ | 8GB+ | Or CPU (slower) |
| Captioning (Qwen3-VL) | RTX 3090 | 24GB | bf16, ~2 img/sec |
| Captioning (4-bit) | RTX 3060 | 12GB | Quantized |
| Vision Training (QLoRA) | RTX 3090 | 24GB | Local option |
| Vision Training (Full) | A100 | 80GB | HF AutoTrain recommended |

**Note:** FaceVault and captioning both support CPU fallback, but GPU is much faster.

---

## Use Cases

### Personal Photo Library
1. Use FaceVault to organize family photos
2. Export faces for each person
3. Train personalized LoRA models
4. Generate custom artwork of family members

### Content Creation
1. Organize stock photos by subject
2. Caption with custom taxonomy
3. Train specialized vision model
4. Generate captions for new content

### Research & Training
1. Collect domain-specific images
2. Caption with controlled vocabulary
3. Fine-tune vision models
4. Create specialized captioning tools

### Dataset Curation
1. Download raw datasets from HuggingFace
2. Filter and organize with FaceVault
3. Caption with quality control
4. Re-upload curated dataset

---

## Roadmap

### FaceVault
- [ ] Web UI for face review/labeling
- [ ] Advanced quality metrics (blur, occlusion)
- [ ] Similarity search
- [ ] Manual cluster merge/split
- [ ] Direct cloud uploads

### Vision Pipeline
- [ ] Universal dataset loader (HF, Kaggle, local)
- [ ] Multiple captioning model support
- [ ] Interactive caption editing/refinement
- [ ] Integrated LoRA training
- [ ] Quality scoring and filtering
- [ ] Multi-GPU support

### Integration
- [ ] One-command end-to-end workflow
- [ ] Preset configs for common use cases
- [ ] Automatic taxonomy generation
- [ ] Model evaluation tools

---

## Contributing

This is a personal toolkit, but contributions are welcome!

**Areas for improvement:**
- Additional export formats
- New captioning models
- Training integrations
- Documentation improvements
- Bug fixes

---

## License

MIT License - Free for personal and commercial use.

---

## Credits

**Tools & Libraries:**
- [InsightFace](https://github.com/deepinsight/insightface) - Face recognition
- [Qwen3-VL](https://huggingface.co/Qwen) - Vision-language model
- [Rich](https://github.com/Textualize/rich) - Terminal UI
- [Click](https://click.palletsprojects.com/) - CLI framework
- [HuggingFace](https://huggingface.co/) - Model hub & training

**Inspiration:**
- [p-e-w/heretic](https://github.com/p-e-w/heretic) - Simple, powerful CLI UX

---

**Built with â¤ï¸ for the joy of creating personalized AI**

Perfect for organizing Christmas photos, training custom models, or building the next generation of vision AI! ğŸ„ğŸ¦«
